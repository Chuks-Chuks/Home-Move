FROM apache/airflow:2.8.0-python3.10

# Force proper Bash installation
USER root
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        bash \
        coreutils && \
    ln -sf bash /bin/sh && \
    rm -rf /var/lib/apt/lists/* && \
    echo "Bash installed at: $(which bash)" && \
    bash --version

# Spark installation
COPY spark/spark-3.5.0-bin-hadoop3.tgz /tmp/
RUN tar xzf /tmp/spark-3.5.0-bin-hadoop3.tgz -C /opt && \
    mv /opt/spark-3.5.0-bin-hadoop3 /opt/spark && \
    rm /tmp/spark-3.5.0-bin-hadoop3.tgz

# Environment setup
ENV SPARK_HOME=/opt/spark \
    PATH="/opt/spark/bin:${PATH}" \
    SHELL=/bin/bash

USER airflow

RUN pip install --no-cache-dir \
    pyspark==3.5.0 \
    pyarrow==14.0.2 \
    pandas==2.1.4 \
    apache-airflow-providers-apache-spark==4.3.0

# Environment setup
ENV SPARK_HOME=/opt/spark \
    PATH="/opt/spark/bin:${PATH}" \
    PYTHONPATH="/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip"